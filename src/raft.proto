option (int32_type) = int_t;

package raft;

// OVERVIEW 
// ----------------------------------------------------------------------------
//
// The Raft protocol defines: 
// * 4 messages 
// * 1 server state with 3 mutually exclusive roles. 
// * 2 Timeout events 
//
// The server has always one role assigned and can 
// receive any message at any time. Timeout events 
// as we will see later are specific to roles.
 
// MESSAGES
// ----------------------------------------------------------------------------
//
// Messages define the data exchanged between the servers participating
// in the RAFT protocol. The messages described below don't include 
// the protocol between a client application and the RAFT servers. Those
// messages would be specific to a concrete implementation.
//
// The 4 Raft messages are divided into 2 request/response pairs. 
//
// - Request Vote
//   The [Request Vote] messages are used for leader election. The RAFT 
//   protocol relies on a [Leader] server to coordinate the state modifications 
//   and ensuring replication to all the other servers. 
//
// - Append Entries
//   The [Append Entries] messages are used for replication of the 
//   data between the [Leader] and the [Follower] servers. 
//   Additionally empty [AppendEntriesRequet]s are used for 
//   heartbeat notifications.

// The [Message] type groups together the 4 messages that makes 
// up the RAFT protocol. 
message Message {
    oneof t {
        RequestVoteRequest    request_vote_request    = 1;
        RequestVoteResponse   request_vote_response   = 2;
        AppendEntriesRequest  append_entries_request  = 3;
        AppendEntriesResponse append_entries_response = 4;
    }
}

// [RequestVoteRequest] is sent by a [Candidate] server 
// to ask another server to vote for itself. 
//
// This message is sent when a New Election Timeout event happened 
// and the server transitions from [Follower] to [Candidate]. 
// 
// [candidate_term] identifies the election term. The term uniquely
// identifies an election round. 
//
// [candidate_last_log_index] and [candidate_last_log_term] indicates
// the most recent data replicated by the [Candidate ]sender. This 
// information is needed by the receiver of the message to determine if the 
// candidate is up to date. 
// 
message RequestVoteRequest {
    required int32 candidate_term           = 1;
    required int32 candidate_id             = 2;
    required int32 candidate_last_log_index = 3; 
    required int32 candidate_last_log_term  = 4;
}

// [RequestVoteResponse] is the message expected to be sent by a server 
// in reply to a [RequestVoteRequest]. 
// 
// In short, it indicates whether or not the server has granted 
// its vote for the requested election.
//
message RequestVoteResponse {
    required int32 voter_id     = 1; 
    required int32 voter_term   = 2; 
    required bool  vote_granted = 3; 
}

// [AppendEntriesRequest] is sent by a [Leader] server to 
// replicate data to the other servers. 
// 
// While data replication is the primary purpose of this message,
// it also has additional functionality. 
//
// Establishing Leadersip:
//   Because only a [Leader] can send such a message, this message
//   type is also an indication to other servers that the sender
//   is the current [Leader]. Therefore a freshly elected server (ie which 
//   transitioned from Candidate to Leader after receiving the majority
//   of votes) will immediately send an [AppendEntriesRequest] to establish
//   its leadership to the other servers. 
// 
// Heartbeat:
//   In order for a leader to maintain its [Leader] role it must 
//   continuously remind other servers of its current leadership. In fact
//   if the [Leader] fails to do so, another server will eventually believe 
//   that the Leader is no longer assuming its role. That server will
//   consequently initiate a new election.
//   The [Leader] is therefore sending empty [AppendEntriesRequest] at regular 
//   time intervals to perpetuate its [Leader] role.
//
// [leader_term]/[leader_id] identifies the server. The [leader_term] is 
// also used by the receiver of the message to either be aware of a new term or 
// detect that Leader is from an older term and therefore reject the request. 
// 
// [prev_log_index]/[prev_log_term] variables keep track of the [Leader] belief about  
// the last replicated data on the receiver. This information is crucial for 
// syncing together a [Leader] and a [Follower]. 
// This is particularly true at the time a server becomes a [Leader]; it 
// can only guess the replicated state on the other servers. 
// In fact it first assumes that all other servers have replicated the exact
// same data as it did. This is most likely incorrect and therefore the first 
// interactions of a [Leader] and its [Followers] would rectify this.
//
// [rev_log_entries] contains the data to be replicated. It's in reverse
// order for performance reasons only. (See note on Log Entries ordering). 
// 
// [leader_commit] indicates the latest data replicated by a majority of 
// servers. This information is helpful for the receiver since it can now
// assume that any data up to that index will be persisted forever.
//
message AppendEntriesRequest {
    required int32 leader_term         = 1; 
    required int32 leader_id           = 2; 
    required int32 prev_log_index      = 3; 
    required int32 prev_log_term       = 4; 
    repeated LogEntry rev_log_entries  = 5;
    required int32 leader_commit       = 6; 
}

// [AppendEntriesResponse] is the message sent by a server in response to
// [AppendEntriesRequest]. 
// 
// Besides the [receiver_id] and [receiver_term] of the message sender, the main
// content of the message is the success or failure of the append operation.
//
// In the case it was successful, the server sends its 
// [receiver_last_log_index] so that the [Leader] can update its own 
// belief of what was the last replicated data on that server. 
//
// A failure can happen for 2 reasons:
//
// - Outdated Leader Term. 
//   It is possible that a [Leader] has already been replaced by 
//   another server during a new election. In such a case the [leader_term]
//   will be less than the [receiver_term]. Since the response 
//   contains the [receiver_term], this outdated [Leader] will
//   realize it should no longer assume such a role and will transition to 
//   becoming a [Follower] in the current term.
// 
// - Invalid [prev_log_index]/[prev_log_index]
//   As previously mentioned in the [AppendEntriesRequest] it is possible
//   that the [Leader] belief of the latest replicated data on the server
//   is incorrect. 
// 
message AppendEntriesResponse {
    required int32 receiver_id      = 1; 
    required int32 receiver_term    = 2; 
    
    oneof result {
      SuccessData success         = 4; 
      LogFailureData log_failure  = 5; 
        // This failure happens in the case the [prev_log_index] and 
        // [prev_log_term] in the [AppendEntriesRequet] are not 
        // matching the receiver log. 

      NoData      term_failure   = 6; 
        // This failure can happen because the sender [current_term]
        // is outdated compared to the receiver.
    }

    message NoData {
    }

    message LogFailureData {
      required int32 receiver_last_log_index = 1; 
      required int32 receiver_last_log_term  = 2;
    }

    message SuccessData {
      required int32 receiver_last_log_index = 1; 
    } 
}

// STATE
// ----------------------------------------------------------------------------
// 
// In this section we will define the [State] of the servers. The [State] is 
// composed into 2 parts
//
// - Common data
//   This data is common to all the servers no matter what role they have.
//
// - Role based data
//   Data specific to a particular role

// State structure
//
// [id] 
//   Id the logical id of the server. Its range is `[0; nb_of_server[`. 
//     
// [current_term]
//   TODO.
// 
// [log] & [log_size]
//   Data to be replicated by the RAFT consensus.
//   [log_size] is simply an optimization, since [log] is represented
//   as an OCaml list, computing the length is a linear operation.
//
// [commit_index]
//   It's the latest log index which has been successfully replicated 
//   on a majority of servers. 
// 
// [configuration] 
//   Cluster configuration (number of servers and various timeout 
//   values). 
// 
// [role] 
//   The current role of the server. [oneof] guarantees that 
//   the server can only be in one role at a time.
//
message State {
  required int32 id           = 1;
  required int32 current_term = 2; 
  repeated LogEntry log       = 3; 
  required int32 log_size     = 10;
  required int32 commit_index = 4; 

  oneof role {
    LeaderState    leader    = 6;
    CandidateState candidate = 7;
    FollowerState  follower  = 8;
  }

  required Configuration configuration = 9;
}

// [LogEntry] 
// 
// RAFT protocol ensures consensus on the execution of a 
// state machine. In other words each RAFT server must eventually
// execute the exact same state machine instructions, hence leading to the
// same state.
//
// Each instruction of the state machine is wrapped into a [LogEntry]. 
// 
// [data] 
//   Placeholder for the application state machine instruction. 
//
// [index]
//   Instruction index. Each newly added instruction increments by 1 
//   the previous instruction index. 
//
// [term]
//   During which term the log entry was inserted into the state [Log].
//
message LogEntry {
  required int32 index = 1; // starts at 1  
  required int32 term  = 2; // starts at 0
  required bytes data  = 3; 
}

// [LeaderState]
//
// When a server is a [Leader] it must keep tracks of several 
// information in order to do its job. 
//
// [next_index]
//   Which [LogEntry] should be sent next to a particular 
//   server. This information is used when the [Leader] computes 
//   the [AppendEntriesRequest] for a [Follower]: it 
//   will send all the [LogEntry] with index >= to that index. 
// 
// [match_index] 
//   The index of the last known replicated [LogEntry]. This 
//   information is used by the [Leader] to know if a particular
//   [LogEntry] has been replicated by a majority of server and 
//   can therefore be commited.
//
// [receiver_heartbeats]
//   In order to maintain its leadership the [Leader] must 
//   regularly send [AppendEntriesRequest]s to all the [Follower]s.
//   [receiver_heartbeats] variable keeps track of when should the next 
//   heartbeat message be sent.
//  
message LeaderState {
  repeated ServerIndex        next_index          = 1;  
  repeated ServerIndex        match_index         = 2;  
  repeated ReceiverConnection receiver_connections= 3;
}

// [CandidateState]
//
// When a server is a [Candidate] it must keep track of its 
// [vote_count] so that when it reaches a majority of votes it 
// becomes a leader. 
// 
// Additionally each election can only last for so long (based 
// on the configuration) and therefore the [Candidate] must keep
// track of when the election should end ([election_deadline]). 
//
// It's important to note that a [Candidate] will only start a 
// new election after the [election_deadline]. Event if it 
// receives a majority of negative responses it has to wait until then.
// This gives the chance to another server to create a new election.
//
message CandidateState {
  required int32 vote_count         = 1;
  required double election_deadline = 2;
}

// [FollowerState]
//
// When a server is a [Follower] it first keeps track of its [Leader].
// This information is optional because during an election there are no
// [Leader]s. The [Follower] keeps track of this information so that 
// if a client applcation is sending a request to it, it can redirect 
// the client to the appropriate [Leader] server.
//
// The [Follower] is also keeping track of which [Candidate] it voted
// for during its current term. This is to ensure that a [Follower] can 
// only vote once.
// 
// Finally the [Follower] also sets an [election_deadline]. If it has 
// not receive any [AppendEntriesRequest] from a valid [Leader] until
// [election_deadline], it will then become a [Candidate] for the next 
// term. 
// We can see now how crucial it is for the [Leader] to send regular
// heartbeat messages to the [Follower]. Each time an [AppendEntriesRequest]
// is received by the [Follower] it will reset its [election_timeout]. 
//
message FollowerState {
  optional int32 voted_for          = 1; 
  optional int32 current_leader     = 2;
  required double election_deadline = 3;
}

// [ReceiverConnection] 
// 
// Data structure to keep track of the connection state for 
// a particular receiver. 
// 
// [heartbeat_deadline] is used to keep track of when should 
//   the next heartbeat sent. 
// 
// [outstanding_request] is used to keep track if a request 
//   was previously sent to this server but no request was received
//   yet.
// 
message ReceiverConnection {
  required int32 server_id           = 1; 
  required float heartbeat_deadline  = 2;
  required bool  outstanding_request = 3;
}

message ServerIndex {
  required int32 server_id        = 1; 
  required int32 server_log_index = 2; 
}

// [Configuration] 
// 
// The RAFT protocol can be configured based on the 
// application needs. 
// 
// First the [nb_of_server] that makes up the cluster. 
//
// Second new elections are configured with 2 parameters:
//
// [election_timeout]
//   This is the average time that it will take a server
//   to start an election after receiving the last request
//   from the [Leader]. 
//
// [election_timeout_range]
//   In order to guarantee a stable and fast election
//   mechanism it's has been demonstrated that the election
//   timeout must be randomized. This avoid race condition 
//   were multiple servers become Candidate at the same
//   time. Each new election timeout is calculated using the 
//   following formula:
//   `timeout = election_timeout + Random[-range/2 , +range/2]` 
//
// [hearbeat_timeout]
//   This is the maximum amount of time that a [Leader] must
//   wait between 2 [AppendEntriesRequest]. For the RAFT 
//   protocol to be stable: 
//     hearbeat_timeout << election_timeout 
// 
// [max_nb_logs_per_message]
//   Throttling of the number of message to be sent per
//   [AppendEntriesRequest].
// 
message Configuration {
  required int32  nb_of_server           = 1;
  required double election_timeout       = 2;  
  required double election_timeout_range = 3;
    // The election timeout follows a uniform distribution 
    // centered on [election_timeout] and of range
    // [election_timeout_range]. 
  required double hearbeat_timeout       = 4; 
  required int32 max_nb_logs_per_message = 5;
}

// TIMEOUT EVENTS
// ----------------------------------------------------------------------------
// 
// A RAFT server should re-act to 3 categories of event
//
// - Client Request
//   When a client application contacts a server to append an entry.
//
// - Server Message
//   When another RAFT servers sent a message. This could be any 
//   of the 4 messages defined previously
//
// - Timeout Event
//   The RAFT protocol requires each server to set appropriate 
//   timeout and defines specific action to be taken by the 
//   server in the case the timeout event happened.
//
// The RAFT protocol defines 2 types of timeout:
// 
// - New Election
//   When this timeout happened the server starts a new election.
//   Only [Follower] and [Candidate] can set those timeouts. 
//
// - Heartbeat 
//   When this timout happened it means that one or more servers
//   have not received a request from the [Leader] for the maximum 
//   allowed time. The server (which can only be a [Leader]) must
//   then send a new [AppendEntriesRequest].  
//
message TimeoutEvent {
  enum TimeOutType {
    NEW_LEADER_ELECTION = 1;
    HEARTBEAT           = 2; 
  }
  required double      timeout      = 1;
  required TimeOutType timeout_type = 2;
}

message RevLogCache {
  required int32    prev_index    = 1; 
  required int32    prev_term     = 2; 
  repeated LogEntry rev_log_entries = 3;  
  required int32    last_index    = 4;
}

// Notes 
// ---------------

// - Ordering of Log Entries -
//  
// The default ordering of log entries is descending chronological
// order. The latest log entry (with the highest index) is first
// in the list. 
// 
// The reason for this ordering is to provide efficient usage of the 
// list data structure. 
//
// If the log are ordered in reverse order then the field will be 
// prefixed with [prev]. 
//
